
==> Audit <==
|------------|--------------------------------|----------|---------------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |     User      | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|---------------|---------|---------------------|---------------------|
| start      |                                | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 10:21 IST |                     |
| start      |                                | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 10:46 IST |                     |
| start      |                                | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 10:47 IST | 06 Apr 25 10:51 IST |
| docker-env | minikube docker-env            | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 10:55 IST | 06 Apr 25 10:55 IST |
| docker-env | minikube docker-env            | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 10:56 IST | 06 Apr 25 10:56 IST |
| docker-env | minikube docker-env            | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 10:56 IST | 06 Apr 25 10:56 IST |
| docker-env | minikube docker-env            | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 11:02 IST | 06 Apr 25 11:02 IST |
| service    | minikube service               | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 11:06 IST |                     |
|            | hello-flask-change-service     |          |               |         |                     |                     |
|            | --url                          |          |               |         |                     |                     |
| service    | hello-flask-change-service     | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 11:07 IST |                     |
|            | --url                          |          |               |         |                     |                     |
| service    | hello-flask-change-service     | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 11:07 IST |                     |
| start      |                                | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 11:16 IST | 06 Apr 25 11:17 IST |
| docker-env | minikube docker-env            | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 11:17 IST | 06 Apr 25 11:17 IST |
| service    | hello-flask-change-service     | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 11:19 IST |                     |
|            | --url                          |          |               |         |                     |                     |
| service    | hello-flask-change-service     | minikube | ABHINAV\abhin | v1.35.0 | 06 Apr 25 11:20 IST |                     |
|------------|--------------------------------|----------|---------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/04/06 11:16:05
Running on machine: Abhinav
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0406 11:16:05.363826   18080 out.go:345] Setting OutFile to fd 104 ...
I0406 11:16:05.364841   18080 out.go:397] isatty.IsTerminal(104) = true
I0406 11:16:05.364841   18080 out.go:358] Setting ErrFile to fd 108...
I0406 11:16:05.364841   18080 out.go:397] isatty.IsTerminal(108) = true
W0406 11:16:05.380945   18080 root.go:314] Error reading config file at C:\Users\abhin\.minikube\config\config.json: open C:\Users\abhin\.minikube\config\config.json: The system cannot find the file specified.
I0406 11:16:05.405715   18080 out.go:352] Setting JSON to false
I0406 11:16:05.412262   18080 start.go:129] hostinfo: {"hostname":"Abhinav","uptime":123,"bootTime":1743918241,"procs":319,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.3476 Build 26100.3476","kernelVersion":"10.0.26100.3476 Build 26100.3476","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"c1c70872-2fcc-4ba7-9fd0-e168cdce1512"}
W0406 11:16:05.412262   18080 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0406 11:16:05.415983   18080 out.go:177] ðŸ˜„  minikube v1.35.0 on Microsoft Windows 11 Home Single Language 10.0.26100.3476 Build 26100.3476
I0406 11:16:05.418477   18080 notify.go:220] Checking for updates...
I0406 11:16:05.436430   18080 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0406 11:16:05.438540   18080 driver.go:394] Setting default libvirt URI to qemu:///system
I0406 11:16:05.529473   18080 docker.go:123] docker version: linux-27.5.1:Docker Desktop 4.38.0 (181591)
I0406 11:16:05.538465   18080 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0406 11:16:08.006588   18080 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.4681239s)
I0406 11:16:08.009485   18080 info.go:266] docker info: {ID:98aadce4-0788-485f-ab60-68b586a77223 Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:56 OomKillDisable:true NGoroutines:85 SystemTime:2025-04-06 05:46:07.981145568 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8186417152 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0406 11:16:08.010446   18080 out.go:177] âœ¨  Using the docker driver based on existing profile
I0406 11:16:08.012925   18080 start.go:297] selected driver: docker
I0406 11:16:08.012925   18080 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\abhin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0406 11:16:08.012925   18080 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0406 11:16:08.048187   18080 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0406 11:16:08.301688   18080 info.go:266] docker info: {ID:98aadce4-0788-485f-ab60-68b586a77223 Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:56 OomKillDisable:true NGoroutines:85 SystemTime:2025-04-06 05:46:08.282929353 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8186417152 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0406 11:16:08.375765   18080 cni.go:84] Creating CNI manager for ""
I0406 11:16:08.376292   18080 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0406 11:16:08.376292   18080 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\abhin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0406 11:16:08.377330   18080 out.go:177] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I0406 11:16:08.377851   18080 cache.go:121] Beginning downloading kic base image for docker with docker
I0406 11:16:08.378396   18080 out.go:177] ðŸšœ  Pulling base image v0.0.46 ...
I0406 11:16:08.379477   18080 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0406 11:16:08.380005   18080 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0406 11:16:08.380005   18080 preload.go:146] Found local preload: C:\Users\abhin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0406 11:16:08.380530   18080 cache.go:56] Caching tarball of preloaded images
I0406 11:16:08.381571   18080 preload.go:172] Found C:\Users\abhin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0406 11:16:08.382090   18080 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0406 11:16:08.382614   18080 profile.go:143] Saving config to C:\Users\abhin\.minikube\profiles\minikube\config.json ...
I0406 11:16:08.554589   18080 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0406 11:16:08.556714   18080 localpath.go:146] windows sanitize: C:\Users\abhin\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\abhin\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0406 11:16:08.556714   18080 localpath.go:146] windows sanitize: C:\Users\abhin\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\abhin\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0406 11:16:08.556714   18080 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0406 11:16:08.557769   18080 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0406 11:16:08.557769   18080 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0406 11:16:08.557769   18080 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0406 11:16:08.557769   18080 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0406 11:16:08.557769   18080 localpath.go:146] windows sanitize: C:\Users\abhin\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\abhin\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0406 11:16:44.394175   18080 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0406 11:16:44.397738   18080 cache.go:227] Successfully downloaded all kic artifacts
I0406 11:16:44.401794   18080 start.go:360] acquireMachinesLock for minikube: {Name:mk1f0dca9bebb9524c2545f832004eaa17de29e0 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0406 11:16:44.402367   18080 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0406 11:16:44.402367   18080 start.go:96] Skipping create...Using existing machine configuration
I0406 11:16:44.403037   18080 fix.go:54] fixHost starting: 
I0406 11:16:44.430259   18080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0406 11:16:44.491838   18080 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0406 11:16:44.491838   18080 fix.go:138] unexpected machine state, will restart: <nil>
I0406 11:16:44.492389   18080 out.go:177] ðŸ”„  Restarting existing docker container for "minikube" ...
I0406 11:16:44.511989   18080 cli_runner.go:164] Run: docker start minikube
I0406 11:16:45.079628   18080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0406 11:16:45.229814   18080 kic.go:430] container "minikube" state is running.
I0406 11:16:45.266675   18080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0406 11:16:45.502720   18080 profile.go:143] Saving config to C:\Users\abhin\.minikube\profiles\minikube\config.json ...
I0406 11:16:45.504318   18080 machine.go:93] provisionDockerMachine start ...
I0406 11:16:45.528796   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:45.677085   18080 main.go:141] libmachine: Using SSH client type: native
I0406 11:16:45.711669   18080 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x805360] 0x807ea0 <nil>  [] 0s} 127.0.0.1 50608 <nil> <nil>}
I0406 11:16:45.711669   18080 main.go:141] libmachine: About to run SSH command:
hostname
I0406 11:16:46.132706   18080 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0406 11:16:46.141127   18080 ubuntu.go:169] provisioning hostname "minikube"
I0406 11:16:46.159538   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:46.240850   18080 main.go:141] libmachine: Using SSH client type: native
I0406 11:16:46.241423   18080 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x805360] 0x807ea0 <nil>  [] 0s} 127.0.0.1 50608 <nil> <nil>}
I0406 11:16:46.241423   18080 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0406 11:16:46.547295   18080 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0406 11:16:46.556382   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:46.630411   18080 main.go:141] libmachine: Using SSH client type: native
I0406 11:16:46.631014   18080 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x805360] 0x807ea0 <nil>  [] 0s} 127.0.0.1 50608 <nil> <nil>}
I0406 11:16:46.631014   18080 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0406 11:16:46.916461   18080 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0406 11:16:46.916991   18080 ubuntu.go:175] set auth options {CertDir:C:\Users\abhin\.minikube CaCertPath:C:\Users\abhin\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\abhin\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\abhin\.minikube\machines\server.pem ServerKeyPath:C:\Users\abhin\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\abhin\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\abhin\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\abhin\.minikube}
I0406 11:16:46.916991   18080 ubuntu.go:177] setting up certificates
I0406 11:16:46.916991   18080 provision.go:84] configureAuth start
I0406 11:16:46.927737   18080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0406 11:16:46.983768   18080 provision.go:143] copyHostCerts
I0406 11:16:47.013578   18080 exec_runner.go:144] found C:\Users\abhin\.minikube/key.pem, removing ...
I0406 11:16:47.013578   18080 exec_runner.go:203] rm: C:\Users\abhin\.minikube\key.pem
I0406 11:16:47.013578   18080 exec_runner.go:151] cp: C:\Users\abhin\.minikube\certs\key.pem --> C:\Users\abhin\.minikube/key.pem (1679 bytes)
I0406 11:16:47.037396   18080 exec_runner.go:144] found C:\Users\abhin\.minikube/ca.pem, removing ...
I0406 11:16:47.037396   18080 exec_runner.go:203] rm: C:\Users\abhin\.minikube\ca.pem
I0406 11:16:47.038133   18080 exec_runner.go:151] cp: C:\Users\abhin\.minikube\certs\ca.pem --> C:\Users\abhin\.minikube/ca.pem (1074 bytes)
I0406 11:16:47.063474   18080 exec_runner.go:144] found C:\Users\abhin\.minikube/cert.pem, removing ...
I0406 11:16:47.063474   18080 exec_runner.go:203] rm: C:\Users\abhin\.minikube\cert.pem
I0406 11:16:47.063474   18080 exec_runner.go:151] cp: C:\Users\abhin\.minikube\certs\cert.pem --> C:\Users\abhin\.minikube/cert.pem (1119 bytes)
I0406 11:16:47.063980   18080 provision.go:117] generating server cert: C:\Users\abhin\.minikube\machines\server.pem ca-key=C:\Users\abhin\.minikube\certs\ca.pem private-key=C:\Users\abhin\.minikube\certs\ca-key.pem org=abhin.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0406 11:16:47.172292   18080 provision.go:177] copyRemoteCerts
I0406 11:16:47.175296   18080 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0406 11:16:47.183154   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:47.289313   18080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50608 SSHKeyPath:C:\Users\abhin\.minikube\machines\minikube\id_rsa Username:docker}
I0406 11:16:47.402228   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0406 11:16:47.426296   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0406 11:16:47.447104   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0406 11:16:47.470695   18080 provision.go:87] duration metric: took 553.1856ms to configureAuth
I0406 11:16:47.470695   18080 ubuntu.go:193] setting minikube options for container-runtime
I0406 11:16:47.471230   18080 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0406 11:16:47.483146   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:47.536470   18080 main.go:141] libmachine: Using SSH client type: native
I0406 11:16:47.536470   18080 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x805360] 0x807ea0 <nil>  [] 0s} 127.0.0.1 50608 <nil> <nil>}
I0406 11:16:47.536470   18080 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0406 11:16:47.688795   18080 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0406 11:16:47.688795   18080 ubuntu.go:71] root file system type: overlay
I0406 11:16:47.689114   18080 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0406 11:16:47.702855   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:47.760130   18080 main.go:141] libmachine: Using SSH client type: native
I0406 11:16:47.760650   18080 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x805360] 0x807ea0 <nil>  [] 0s} 127.0.0.1 50608 <nil> <nil>}
I0406 11:16:47.760650   18080 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0406 11:16:47.918759   18080 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0406 11:16:47.934740   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:47.996210   18080 main.go:141] libmachine: Using SSH client type: native
I0406 11:16:47.996210   18080 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x805360] 0x807ea0 <nil>  [] 0s} 127.0.0.1 50608 <nil> <nil>}
I0406 11:16:47.996210   18080 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0406 11:16:48.141616   18080 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0406 11:16:48.141616   18080 machine.go:96] duration metric: took 2.6372976s to provisionDockerMachine
I0406 11:16:48.142222   18080 start.go:293] postStartSetup for "minikube" (driver="docker")
I0406 11:16:48.142222   18080 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0406 11:16:48.145963   18080 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0406 11:16:48.154184   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:48.198193   18080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50608 SSHKeyPath:C:\Users\abhin\.minikube\machines\minikube\id_rsa Username:docker}
I0406 11:16:48.330387   18080 ssh_runner.go:195] Run: cat /etc/os-release
I0406 11:16:48.335576   18080 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0406 11:16:48.335576   18080 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0406 11:16:48.335576   18080 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0406 11:16:48.335576   18080 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0406 11:16:48.336104   18080 filesync.go:126] Scanning C:\Users\abhin\.minikube\addons for local assets ...
I0406 11:16:48.336627   18080 filesync.go:126] Scanning C:\Users\abhin\.minikube\files for local assets ...
I0406 11:16:48.336627   18080 start.go:296] duration metric: took 194.4052ms for postStartSetup
I0406 11:16:48.382330   18080 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0406 11:16:48.393362   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:48.442290   18080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50608 SSHKeyPath:C:\Users\abhin\.minikube\machines\minikube\id_rsa Username:docker}
I0406 11:16:48.577853   18080 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0406 11:16:48.584913   18080 fix.go:56] duration metric: took 4.1818758s for fixHost
I0406 11:16:48.584913   18080 start.go:83] releasing machines lock for "minikube", held for 4.1825461s
I0406 11:16:48.596928   18080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0406 11:16:48.650592   18080 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0406 11:16:48.668970   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:48.712271   18080 ssh_runner.go:195] Run: cat /version.json
I0406 11:16:48.723934   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:48.750598   18080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50608 SSHKeyPath:C:\Users\abhin\.minikube\machines\minikube\id_rsa Username:docker}
I0406 11:16:48.780259   18080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50608 SSHKeyPath:C:\Users\abhin\.minikube\machines\minikube\id_rsa Username:docker}
W0406 11:16:48.845925   18080 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0406 11:16:48.928359   18080 ssh_runner.go:195] Run: systemctl --version
I0406 11:16:48.982150   18080 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0406 11:16:48.992208   18080 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0406 11:16:49.003964   18080 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0406 11:16:49.007920   18080 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0406 11:16:49.021099   18080 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0406 11:16:49.021099   18080 start.go:495] detecting cgroup driver to use...
I0406 11:16:49.021099   18080 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0406 11:16:49.022881   18080 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0406 11:16:49.095022   18080 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0406 11:16:49.296291   18080 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0406 11:16:49.318975   18080 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0406 11:16:49.357974   18080 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0406 11:16:49.503637   18080 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0406 11:16:49.567676   18080 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
W0406 11:16:49.578559   18080 out.go:270] â—  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0406 11:16:49.578559   18080 out.go:270] ðŸ’¡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0406 11:16:49.628919   18080 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0406 11:16:49.769995   18080 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0406 11:16:49.820978   18080 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0406 11:16:49.867369   18080 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0406 11:16:49.914638   18080 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0406 11:16:49.930162   18080 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0406 11:16:49.943423   18080 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0406 11:16:49.955254   18080 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0406 11:16:50.052971   18080 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0406 11:16:50.193125   18080 start.go:495] detecting cgroup driver to use...
I0406 11:16:50.193125   18080 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0406 11:16:50.197514   18080 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0406 11:16:50.210316   18080 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0406 11:16:50.215231   18080 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0406 11:16:50.229004   18080 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0406 11:16:50.317420   18080 ssh_runner.go:195] Run: which cri-dockerd
I0406 11:16:50.329733   18080 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0406 11:16:50.340678   18080 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0406 11:16:50.358311   18080 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0406 11:16:50.488625   18080 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0406 11:16:50.569131   18080 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0406 11:16:50.569131   18080 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0406 11:16:50.591416   18080 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0406 11:16:50.682982   18080 ssh_runner.go:195] Run: sudo systemctl restart docker
I0406 11:16:54.685332   18080 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.00235s)
I0406 11:16:54.690365   18080 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0406 11:16:54.706556   18080 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0406 11:16:54.722752   18080 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0406 11:16:54.739752   18080 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0406 11:16:54.842109   18080 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0406 11:16:54.963107   18080 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0406 11:16:55.061336   18080 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0406 11:16:55.079037   18080 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0406 11:16:55.093429   18080 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0406 11:16:55.198519   18080 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0406 11:16:55.460227   18080 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0406 11:16:55.497832   18080 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0406 11:16:55.503405   18080 start.go:563] Will wait 60s for crictl version
I0406 11:16:55.541806   18080 ssh_runner.go:195] Run: which crictl
I0406 11:16:55.550168   18080 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0406 11:16:55.673311   18080 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0406 11:16:55.687246   18080 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0406 11:16:55.782358   18080 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0406 11:16:55.806860   18080 out.go:235] ðŸ³  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0406 11:16:55.817717   18080 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0406 11:16:55.950548   18080 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0406 11:16:56.000133   18080 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0406 11:16:56.005446   18080 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0406 11:16:56.030329   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0406 11:16:56.087097   18080 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\abhin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0406 11:16:56.087628   18080 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0406 11:16:56.096402   18080 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0406 11:16:56.117085   18080 docker.go:689] Got preloaded images: -- stdout --
my-hello-world-app:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0406 11:16:56.117085   18080 docker.go:619] Images already preloaded, skipping extraction
I0406 11:16:56.125328   18080 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0406 11:16:56.147802   18080 docker.go:689] Got preloaded images: -- stdout --
my-hello-world-app:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0406 11:16:56.147802   18080 cache_images.go:84] Images are preloaded, skipping loading
I0406 11:16:56.148381   18080 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0406 11:16:56.152864   18080 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0406 11:16:56.164163   18080 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0406 11:16:56.347964   18080 cni.go:84] Creating CNI manager for ""
I0406 11:16:56.347964   18080 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0406 11:16:56.348501   18080 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0406 11:16:56.348501   18080 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0406 11:16:56.348501   18080 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0406 11:16:56.351680   18080 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0406 11:16:56.362819   18080 binaries.go:44] Found k8s binaries, skipping transfer
I0406 11:16:56.365003   18080 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0406 11:16:56.374238   18080 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0406 11:16:56.391779   18080 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0406 11:16:56.409236   18080 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0406 11:16:56.466602   18080 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0406 11:16:56.472773   18080 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0406 11:16:56.486505   18080 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0406 11:16:56.543030   18080 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0406 11:16:56.559988   18080 certs.go:68] Setting up C:\Users\abhin\.minikube\profiles\minikube for IP: 192.168.49.2
I0406 11:16:56.559988   18080 certs.go:194] generating shared ca certs ...
I0406 11:16:56.559988   18080 certs.go:226] acquiring lock for ca certs: {Name:mk71314fe3bb38ee877cbe14c4cbe4fc47ce85f2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0406 11:16:56.580403   18080 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\abhin\.minikube\ca.key
I0406 11:16:56.608751   18080 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\abhin\.minikube\proxy-client-ca.key
I0406 11:16:56.608751   18080 certs.go:256] generating profile certs ...
I0406 11:16:56.609843   18080 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\abhin\.minikube\profiles\minikube\client.key
I0406 11:16:56.647955   18080 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\abhin\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0406 11:16:56.679517   18080 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\abhin\.minikube\profiles\minikube\proxy-client.key
I0406 11:16:56.684677   18080 certs.go:484] found cert: C:\Users\abhin\.minikube\certs\ca-key.pem (1675 bytes)
I0406 11:16:56.684677   18080 certs.go:484] found cert: C:\Users\abhin\.minikube\certs\ca.pem (1074 bytes)
I0406 11:16:56.685184   18080 certs.go:484] found cert: C:\Users\abhin\.minikube\certs\cert.pem (1119 bytes)
I0406 11:16:56.685184   18080 certs.go:484] found cert: C:\Users\abhin\.minikube\certs\key.pem (1679 bytes)
I0406 11:16:56.689495   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0406 11:16:56.715842   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0406 11:16:56.741239   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0406 11:16:56.765064   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0406 11:16:56.791341   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0406 11:16:56.818300   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0406 11:16:56.846484   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0406 11:16:56.877072   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0406 11:16:56.908108   18080 ssh_runner.go:362] scp C:\Users\abhin\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0406 11:16:57.015476   18080 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0406 11:16:57.095516   18080 ssh_runner.go:195] Run: openssl version
I0406 11:16:57.113925   18080 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0406 11:16:57.173899   18080 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0406 11:16:57.180231   18080 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Apr  6 05:21 /usr/share/ca-certificates/minikubeCA.pem
I0406 11:16:57.181326   18080 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0406 11:16:57.191819   18080 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0406 11:16:57.245108   18080 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0406 11:16:57.252073   18080 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0406 11:16:57.261661   18080 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0406 11:16:57.272197   18080 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0406 11:16:57.281810   18080 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0406 11:16:57.291780   18080 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0406 11:16:57.300555   18080 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0406 11:16:57.308404   18080 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\abhin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0406 11:16:57.316937   18080 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0406 11:16:57.348248   18080 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0406 11:16:57.358663   18080 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0406 11:16:57.359761   18080 kubeadm.go:593] restartPrimaryControlPlane start ...
I0406 11:16:57.363883   18080 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0406 11:16:57.376097   18080 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0406 11:16:57.386436   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0406 11:16:57.457263   18080 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:57853"
I0406 11:16:57.457828   18080 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:57853, want: 127.0.0.1:50607
I0406 11:16:57.458358   18080 kubeconfig.go:62] C:\Users\abhin\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I0406 11:16:57.458914   18080 lock.go:35] WriteFile acquiring C:\Users\abhin\.kube\config: {Name:mk0c0eab2435e7affdc226b641b89a91ad7121d2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0406 11:16:57.516441   18080 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0406 11:16:57.527860   18080 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0406 11:16:57.527860   18080 kubeadm.go:597] duration metric: took 168.0995ms to restartPrimaryControlPlane
I0406 11:16:57.527860   18080 kubeadm.go:394] duration metric: took 219.4557ms to StartCluster
I0406 11:16:57.528923   18080 settings.go:142] acquiring lock: {Name:mk21737abd6b54072077d3221ce2651c7cdf13cc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0406 11:16:57.528923   18080 settings.go:150] Updating kubeconfig:  C:\Users\abhin\.kube\config
I0406 11:16:57.529961   18080 lock.go:35] WriteFile acquiring C:\Users\abhin\.kube\config: {Name:mk0c0eab2435e7affdc226b641b89a91ad7121d2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0406 11:16:57.531250   18080 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0406 11:16:57.531250   18080 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0406 11:16:57.531810   18080 out.go:177] ðŸ”Ž  Verifying Kubernetes components...
I0406 11:16:57.532369   18080 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0406 11:16:57.533404   18080 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0406 11:16:57.533404   18080 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0406 11:16:57.533926   18080 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0406 11:16:57.533926   18080 addons.go:247] addon storage-provisioner should already be in state true
I0406 11:16:57.533926   18080 host.go:66] Checking if "minikube" exists ...
I0406 11:16:57.533926   18080 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0406 11:16:57.537251   18080 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0406 11:16:57.565079   18080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0406 11:16:57.566135   18080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0406 11:16:57.648269   18080 out.go:177]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0406 11:16:57.648843   18080 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0406 11:16:57.648843   18080 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0406 11:16:57.650494   18080 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0406 11:16:57.650494   18080 addons.go:247] addon default-storageclass should already be in state true
I0406 11:16:57.651057   18080 host.go:66] Checking if "minikube" exists ...
I0406 11:16:57.659844   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:57.677238   18080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0406 11:16:57.785127   18080 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0406 11:16:57.785127   18080 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0406 11:16:57.785127   18080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50608 SSHKeyPath:C:\Users\abhin\.minikube\machines\minikube\id_rsa Username:docker}
I0406 11:16:57.802390   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0406 11:16:57.855974   18080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50608 SSHKeyPath:C:\Users\abhin\.minikube\machines\minikube\id_rsa Username:docker}
I0406 11:16:57.908634   18080 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0406 11:16:58.027927   18080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0406 11:16:58.080903   18080 api_server.go:52] waiting for apiserver process to appear ...
I0406 11:16:58.083602   18080 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0406 11:16:58.113131   18080 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0406 11:16:58.224302   18080 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0406 11:16:58.584791   18080 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0406 11:16:58.813472   18080 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0406 11:16:58.814016   18080 retry.go:31] will retry after 233.757852ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0406 11:16:58.815667   18080 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0406 11:16:58.815667   18080 retry.go:31] will retry after 146.326189ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0406 11:16:58.966526   18080 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0406 11:16:59.052412   18080 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0406 11:16:59.083995   18080 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0406 11:16:59.093967   18080 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0406 11:16:59.093967   18080 retry.go:31] will retry after 498.259358ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0406 11:16:59.204081   18080 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0406 11:16:59.204081   18080 api_server.go:72] duration metric: took 1.6728316s to wait for apiserver process to appear ...
I0406 11:16:59.204081   18080 api_server.go:88] waiting for apiserver healthz status ...
I0406 11:16:59.204081   18080 retry.go:31] will retry after 239.673675ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0406 11:16:59.204081   18080 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50607/healthz ...
I0406 11:16:59.206370   18080 api_server.go:269] stopped: https://127.0.0.1:50607/healthz: Get "https://127.0.0.1:50607/healthz": EOF
I0406 11:16:59.447562   18080 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0406 11:16:59.601012   18080 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0406 11:16:59.704543   18080 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50607/healthz ...
I0406 11:17:01.574671   18080 api_server.go:279] https://127.0.0.1:50607/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0406 11:17:01.574671   18080 api_server.go:103] status: https://127.0.0.1:50607/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0406 11:17:01.574671   18080 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50607/healthz ...
I0406 11:17:01.720337   18080 api_server.go:279] https://127.0.0.1:50607/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0406 11:17:01.720337   18080 api_server.go:103] status: https://127.0.0.1:50607/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0406 11:17:01.720337   18080 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50607/healthz ...
I0406 11:17:01.904589   18080 api_server.go:279] https://127.0.0.1:50607/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0406 11:17:01.904589   18080 api_server.go:103] status: https://127.0.0.1:50607/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0406 11:17:02.205549   18080 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50607/healthz ...
I0406 11:17:02.307819   18080 api_server.go:279] https://127.0.0.1:50607/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0406 11:17:02.307819   18080 api_server.go:103] status: https://127.0.0.1:50607/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0406 11:17:02.704540   18080 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50607/healthz ...
I0406 11:17:02.716476   18080 api_server.go:279] https://127.0.0.1:50607/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0406 11:17:02.716476   18080 api_server.go:103] status: https://127.0.0.1:50607/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0406 11:17:03.204458   18080 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50607/healthz ...
I0406 11:17:03.302154   18080 api_server.go:279] https://127.0.0.1:50607/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0406 11:17:03.302916   18080 api_server.go:103] status: https://127.0.0.1:50607/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0406 11:17:03.704631   18080 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50607/healthz ...
I0406 11:17:03.727850   18080 api_server.go:279] https://127.0.0.1:50607/healthz returned 200:
ok
I0406 11:17:03.917877   18080 api_server.go:141] control plane version: v1.32.0
I0406 11:17:03.917877   18080 api_server.go:131] duration metric: took 4.7137958s to wait for apiserver health ...
I0406 11:17:03.918535   18080 system_pods.go:43] waiting for kube-system pods to appear ...
I0406 11:17:04.026125   18080 system_pods.go:59] 7 kube-system pods found
I0406 11:17:04.026125   18080 system_pods.go:61] "coredns-668d6bf9bc-ngpdq" [2a06e6e2-1d5e-4b96-9557-e023bf8f8c25] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0406 11:17:04.026125   18080 system_pods.go:61] "etcd-minikube" [ac15ba19-06d8-4311-a4a3-412dbdb4279f] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0406 11:17:04.026125   18080 system_pods.go:61] "kube-apiserver-minikube" [5cbe36a7-1d35-442d-bfa3-093d03760cf9] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0406 11:17:04.026125   18080 system_pods.go:61] "kube-controller-manager-minikube" [50fdd4ad-3ab9-42d7-98bd-530daf3ae58f] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0406 11:17:04.026125   18080 system_pods.go:61] "kube-proxy-k58vd" [aac83163-407b-4851-8d24-86c79fe6a6a5] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0406 11:17:04.026125   18080 system_pods.go:61] "kube-scheduler-minikube" [c160882b-ed5f-43bc-8e03-15e703c4c04e] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0406 11:17:04.026125   18080 system_pods.go:61] "storage-provisioner" [a154e8dc-6ee2-4979-98a1-79e10c73347a] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0406 11:17:04.026125   18080 system_pods.go:74] duration metric: took 107.5894ms to wait for pod list to return data ...
I0406 11:17:04.026125   18080 kubeadm.go:582] duration metric: took 6.494875s to wait for: map[apiserver:true system_pods:true]
I0406 11:17:04.026125   18080 node_conditions.go:102] verifying NodePressure condition ...
I0406 11:17:04.113340   18080 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (4.6657781s)
I0406 11:17:04.113340   18080 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (4.5123279s)
I0406 11:17:04.117510   18080 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0406 11:17:04.117510   18080 node_conditions.go:123] node cpu capacity is 12
I0406 11:17:04.118258   18080 node_conditions.go:105] duration metric: took 92.1332ms to run NodePressure ...
I0406 11:17:04.118258   18080 start.go:241] waiting for startup goroutines ...
I0406 11:17:04.317210   18080 out.go:177] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I0406 11:17:04.318494   18080 addons.go:514] duration metric: took 6.7872446s for enable addons: enabled=[storage-provisioner default-storageclass]
I0406 11:17:04.318494   18080 start.go:246] waiting for cluster config update ...
I0406 11:17:04.318494   18080 start.go:255] writing updated cluster config ...
I0406 11:17:04.371737   18080 ssh_runner.go:195] Run: rm -f paused
I0406 11:17:04.672375   18080 start.go:600] kubectl: 1.31.4, cluster: 1.32.0 (minor skew: 1)
I0406 11:17:04.673483   18080 out.go:177] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Apr 06 05:46:52 minikube dockerd[821]: time="2025-04-06T05:46:52.370409451Z" level=info msg="Daemon shutdown complete"
Apr 06 05:46:52 minikube dockerd[821]: time="2025-04-06T05:46:52.370583166Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Apr 06 05:46:52 minikube systemd[1]: docker.service: Deactivated successfully.
Apr 06 05:46:52 minikube systemd[1]: Stopped Docker Application Container Engine.
Apr 06 05:46:52 minikube systemd[1]: Starting Docker Application Container Engine...
Apr 06 05:46:52 minikube dockerd[1107]: time="2025-04-06T05:46:52.406666362Z" level=info msg="Starting up"
Apr 06 05:46:52 minikube dockerd[1107]: time="2025-04-06T05:46:52.407857561Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Apr 06 05:46:52 minikube dockerd[1107]: time="2025-04-06T05:46:52.423558330Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Apr 06 05:46:52 minikube dockerd[1107]: time="2025-04-06T05:46:52.439879687Z" level=info msg="Loading containers: start."
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.176033123Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.592413864Z" level=warning msg="error locating sandbox id f353c4d70c1cf3c73ebf7eb3f48758fc74d253a6cfe0850fae7c4925a64bfa8a: sandbox f353c4d70c1cf3c73ebf7eb3f48758fc74d253a6cfe0850fae7c4925a64bfa8a not found"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.592479044Z" level=warning msg="error locating sandbox id 27ff7e24ac709abbd9d2d22da2f2205ac2553d46d86f1417de8686efdbe7ddc2: sandbox 27ff7e24ac709abbd9d2d22da2f2205ac2553d46d86f1417de8686efdbe7ddc2 not found"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.592494727Z" level=warning msg="error locating sandbox id aebe938b1a651bbe8f473d6831c2f9d1b5a596ca8324e29632fcb374393bb588: sandbox aebe938b1a651bbe8f473d6831c2f9d1b5a596ca8324e29632fcb374393bb588 not found"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.592506274Z" level=warning msg="error locating sandbox id 271a92701ffe33588742b9fdb9927adfd206ae9903e23c82127f3c96c06a7b9a: sandbox 271a92701ffe33588742b9fdb9927adfd206ae9903e23c82127f3c96c06a7b9a not found"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.592519145Z" level=warning msg="error locating sandbox id 23283d2b7269f0d8534a7dc7d7c9e8a159623212280917258bf36360e93e8296: sandbox 23283d2b7269f0d8534a7dc7d7c9e8a159623212280917258bf36360e93e8296 not found"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.592534553Z" level=warning msg="error locating sandbox id f3e95058302f849dc44e1633c5d54d1127a07aba75a4b292eabc20c63c7c1cc9: sandbox f3e95058302f849dc44e1633c5d54d1127a07aba75a4b292eabc20c63c7c1cc9 not found"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.592554886Z" level=warning msg="error locating sandbox id 1f6b0382b41218a2cbc102b89208dec57808d43a37b62147822d299147fef59e: sandbox 1f6b0382b41218a2cbc102b89208dec57808d43a37b62147822d299147fef59e not found"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.592568494Z" level=warning msg="error locating sandbox id 0d899fc5d012bd757bfc1e7ab9fc64201cc738de45ad2f0b86b993e602f0feb9: sandbox 0d899fc5d012bd757bfc1e7ab9fc64201cc738de45ad2f0b86b993e602f0feb9 not found"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.592578866Z" level=warning msg="error locating sandbox id decf4015b7c24194479c07f44ab584d212cb1e5095da92507415201a0a8d5483: sandbox decf4015b7c24194479c07f44ab584d212cb1e5095da92507415201a0a8d5483 not found"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.592591947Z" level=warning msg="error locating sandbox id 2eb8d28e9434ba92733debbb15bafe181dd4e41d22698d9f678d9b1d56cea3c7: sandbox 2eb8d28e9434ba92733debbb15bafe181dd4e41d22698d9f678d9b1d56cea3c7 not found"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.592861627Z" level=info msg="Loading containers: done."
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.607391967Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.607439356Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.607444434Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.607447559Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.607464010Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.607510373Z" level=info msg="Daemon has completed initialization"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.682451062Z" level=info msg="API listen on /var/run/docker.sock"
Apr 06 05:46:54 minikube dockerd[1107]: time="2025-04-06T05:46:54.682483847Z" level=info msg="API listen on [::]:2376"
Apr 06 05:46:54 minikube systemd[1]: Started Docker Application Container Engine.
Apr 06 05:46:55 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Apr 06 05:46:55 minikube cri-dockerd[1401]: time="2025-04-06T05:46:55Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Apr 06 05:46:55 minikube cri-dockerd[1401]: time="2025-04-06T05:46:55Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Apr 06 05:46:55 minikube cri-dockerd[1401]: time="2025-04-06T05:46:55Z" level=info msg="Start docker client with request timeout 0s"
Apr 06 05:46:55 minikube cri-dockerd[1401]: time="2025-04-06T05:46:55Z" level=info msg="Hairpin mode is set to hairpin-veth"
Apr 06 05:46:55 minikube cri-dockerd[1401]: time="2025-04-06T05:46:55Z" level=info msg="Loaded network plugin cni"
Apr 06 05:46:55 minikube cri-dockerd[1401]: time="2025-04-06T05:46:55Z" level=info msg="Docker cri networking managed by network plugin cni"
Apr 06 05:46:55 minikube cri-dockerd[1401]: time="2025-04-06T05:46:55Z" level=info msg="Setting cgroupDriver cgroupfs"
Apr 06 05:46:55 minikube cri-dockerd[1401]: time="2025-04-06T05:46:55Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Apr 06 05:46:55 minikube cri-dockerd[1401]: time="2025-04-06T05:46:55Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Apr 06 05:46:55 minikube cri-dockerd[1401]: time="2025-04-06T05:46:55Z" level=info msg="Start cri-dockerd grpc backend"
Apr 06 05:46:55 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Apr 06 05:46:57 minikube cri-dockerd[1401]: time="2025-04-06T05:46:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-ngpdq_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"219e3e5597cf82b7589a81d4cf94389d2fd9044cf8ba881aeb3fcbd2ed91cea3\""
Apr 06 05:46:57 minikube cri-dockerd[1401]: time="2025-04-06T05:46:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-python-f4bf545-clqf8_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"1185726e2fc3f71f91638a1f4f20e7131db5abf95258882a45871286a82b0f1a\""
Apr 06 05:46:57 minikube cri-dockerd[1401]: time="2025-04-06T05:46:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-python-f4bf545-pd774_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4b951e1b6371fb7095ffcf968c45a3d192882853da048699028b8224de2f384d\""
Apr 06 05:46:57 minikube cri-dockerd[1401]: time="2025-04-06T05:46:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-python-f4bf545-kjgvt_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"87a57b0163caa871c8983cdb3233a8e3f605e3cfdd02efb22c0a3e8c24e919a2\""
Apr 06 05:46:58 minikube cri-dockerd[1401]: time="2025-04-06T05:46:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c5feedd090ea2eebac0f50dcadda3cdffaad803280da8e3920440bfe875531b9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 06 05:46:58 minikube cri-dockerd[1401]: time="2025-04-06T05:46:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6a43213eaa68d32d31c3c4c123d490fc7e9db02ab099f079afd0e818f363fd21/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 06 05:46:58 minikube cri-dockerd[1401]: time="2025-04-06T05:46:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/51b5b7945c3c32fc6c147d0e64d088bc28ebe9ccadf6bd501750cb19b0ed5fd4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 06 05:46:58 minikube cri-dockerd[1401]: time="2025-04-06T05:46:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e8b21b2aa0293d8b165eec5ee2b45796bbe26eb78cfdc3a107b53a35d72be5b2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 06 05:47:01 minikube cri-dockerd[1401]: time="2025-04-06T05:47:01Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Apr 06 05:47:03 minikube cri-dockerd[1401]: time="2025-04-06T05:47:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9d7c975a55cd26c606165d4460f53b8d31522ef94bbca0e4a5da416e19b77e27/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 06 05:47:03 minikube cri-dockerd[1401]: time="2025-04-06T05:47:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cee2bbe37da3d716c8942535d33dfc5f74f9cbbf6b181309d50f452834ff70c9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 06 05:47:04 minikube cri-dockerd[1401]: time="2025-04-06T05:47:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/98f9e78a27421c7d1c9d19827536bda0fc3e94d07c57667ff57402d63deead53/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 06 05:47:04 minikube cri-dockerd[1401]: time="2025-04-06T05:47:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9a6524da6d78c202d813442dbe4d331fe153e5d94e9d6bb398a4d63ae1eca923/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 06 05:47:04 minikube cri-dockerd[1401]: time="2025-04-06T05:47:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f7a0ea30f3db4b6c29b9192c3419a2599f3077904f21b20979bf6bc557878cc3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 06 05:47:05 minikube cri-dockerd[1401]: time="2025-04-06T05:47:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bb1d0cc063bab1ba76cbd4141964c8752bf20e7909ddd009b401b9261b7ce3ab/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 06 05:47:07 minikube dockerd[1107]: time="2025-04-06T05:47:07.069335346Z" level=info msg="ignoring event" container=98f9e78a27421c7d1c9d19827536bda0fc3e94d07c57667ff57402d63deead53 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 06 05:47:07 minikube cri-dockerd[1401]: time="2025-04-06T05:47:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/85066063c37c907772fef348982b27ddeede0334cc147270be89a49d3bb486ca/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 06 05:47:15 minikube dockerd[1107]: time="2025-04-06T05:47:15.628414733Z" level=info msg="ignoring event" container=474c436adfa0b1cf1f5d126b9c356fa3752f77419c2badc56b4fcbe030d77775 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
e49dc451440db       6e38f40d628db       4 minutes ago       Running             storage-provisioner       3                   cee2bbe37da3d       storage-provisioner
b007a700ec124       c69fa2e9cbf5f       5 minutes ago       Running             coredns                   1                   9a6524da6d78c       coredns-668d6bf9bc-ngpdq
474c436adfa0b       6e38f40d628db       5 minutes ago       Exited              storage-provisioner       2                   cee2bbe37da3d       storage-provisioner
86144e9c294b6       040f9f8aac8cd       5 minutes ago       Running             kube-proxy                1                   9d7c975a55cd2       kube-proxy-k58vd
c13c74f2a921c       8cab3d2a8bd0f       5 minutes ago       Running             kube-controller-manager   1                   e8b21b2aa0293       kube-controller-manager-minikube
6b6de4e73268b       a389e107f4ff1       5 minutes ago       Running             kube-scheduler            1                   6a43213eaa68d       kube-scheduler-minikube
7bdf3a07b56b7       c2e17b8d0f4a3       5 minutes ago       Running             kube-apiserver            1                   c5feedd090ea2       kube-apiserver-minikube
72a6e86074c98       a9e7e6b294baf       5 minutes ago       Running             etcd                      1                   51b5b7945c3c3       etcd-minikube
cb6f2c40bd322       c69fa2e9cbf5f       30 minutes ago      Exited              coredns                   0                   219e3e5597cf8       coredns-668d6bf9bc-ngpdq
d7befca2866bd       040f9f8aac8cd       30 minutes ago      Exited              kube-proxy                0                   3f5b21cfb7d37       kube-proxy-k58vd
574cb448b39f4       a9e7e6b294baf       31 minutes ago      Exited              etcd                      0                   35547c835addc       etcd-minikube
e94a8930eece6       a389e107f4ff1       31 minutes ago      Exited              kube-scheduler            0                   e0c29a113eede       kube-scheduler-minikube
1d57265cc2c88       c2e17b8d0f4a3       31 minutes ago      Exited              kube-apiserver            0                   3cac333689d0b       kube-apiserver-minikube
f91627437b1f3       8cab3d2a8bd0f       31 minutes ago      Exited              kube-controller-manager   0                   501e3f9bc9fa3       kube-controller-manager-minikube


==> coredns [b007a700ec12] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:47944 - 55507 "HINFO IN 3450909931954475631.6828920863732089337. udp 57 false 512" NXDOMAIN qr,rd,ra 57 1.081424464s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[210250711]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-Apr-2025 05:47:05.852) (total time: 10010ms):
Trace[210250711]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10010ms (05:47:15.861)
Trace[210250711]: [10.010485821s] [10.010485821s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[381633250]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-Apr-2025 05:47:05.852) (total time: 10011ms):
Trace[381633250]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10010ms (05:47:15.861)
Trace[381633250]: [10.011348655s] [10.011348655s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[465626424]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-Apr-2025 05:47:05.852) (total time: 10011ms):
Trace[465626424]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10010ms (05:47:15.861)
Trace[465626424]: [10.011076821s] [10.011076821s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout


==> coredns [cb6f2c40bd32] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:44385 - 36160 "HINFO IN 2263308920234093508.1709330077114372073. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.051381888s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[856554474]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-Apr-2025 05:21:30.977) (total time: 10006ms):
Trace[856554474]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10006ms (05:21:40.983)
Trace[856554474]: [10.006805572s] [10.006805572s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1383941462]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-Apr-2025 05:21:30.977) (total time: 10006ms):
Trace[1383941462]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10006ms (05:21:40.983)
Trace[1383941462]: [10.006878476s] [10.006878476s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[731695091]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-Apr-2025 05:21:30.977) (total time: 10006ms):
Trace[731695091]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10006ms (05:21:40.983)
Trace[731695091]: [10.00697867s] [10.00697867s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_04_06T10_51_25_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 06 Apr 2025 05:21:22 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 06 Apr 2025 05:52:16 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 06 Apr 2025 05:51:16 +0000   Sun, 06 Apr 2025 05:21:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 06 Apr 2025 05:51:16 +0000   Sun, 06 Apr 2025 05:21:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 06 Apr 2025 05:51:16 +0000   Sun, 06 Apr 2025 05:21:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 06 Apr 2025 05:51:16 +0000   Sun, 06 Apr 2025 05:21:22 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7994548Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7994548Ki
  pods:               110
System Info:
  Machine ID:                 f1dce3e4c0d94d9d89a65ca8f6334a79
  System UUID:                f1dce3e4c0d94d9d89a65ca8f6334a79
  Boot ID:                    7a0e9d24-1b3a-451a-bded-3e435996d767
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     hello-python-f4bf545-clqf8          0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
  default                     hello-python-f4bf545-kjgvt          0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
  default                     hello-python-f4bf545-pd774          0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 coredns-668d6bf9bc-ngpdq            100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     30m
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         30m
  kube-system                 kube-apiserver-minikube             250m (2%)     0 (0%)      0 (0%)           0 (0%)         30m
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         30m
  kube-system                 kube-proxy-k58vd                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         30m
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         30m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         30m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                    From             Message
  ----     ------                             ----                   ----             -------
  Normal   Starting                           30m                    kube-proxy       
  Normal   Starting                           5m18s                  kube-proxy       
  Normal   NodeHasSufficientMemory            31m (x8 over 31m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              31m (x8 over 31m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               31m (x7 over 31m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            31m                    kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            30m                    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                           30m                    kubelet          Starting kubelet.
  Warning  CgroupV1                           30m                    kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            30m                    kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasNoDiskPressure              30m                    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               30m                    kubelet          Node minikube status is now: NodeHasSufficientPID
  Warning  PossibleMemoryBackedVolumesOnDisk  30m                    kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   RegisteredNode                     30m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                           5m28s                  kubelet          Starting kubelet.
  Warning  PossibleMemoryBackedVolumesOnDisk  5m28s                  kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Warning  CgroupV1                           5m28s                  kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            5m27s (x8 over 5m28s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              5m27s (x8 over 5m28s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               5m27s (x7 over 5m28s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            5m27s                  kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                           5m23s                  kubelet          Node minikube has been rebooted, boot id: 7a0e9d24-1b3a-451a-bded-3e435996d767
  Normal   RegisteredNode                     5m19s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Apr 6 05:45] PCI: Fatal: No config space access function found
[  +0.020620] PCI: System does not support PCI
[  +0.143368] kvm: already loaded the other module
[  +1.864300] FS-Cache: Duplicate cookie detected
[  +0.000376] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000357] FS-Cache: O-cookie d=00000000f430af4f{9P.session} n=0000000002d31f4b
[  +0.003242] FS-Cache: O-key=[10] '34323934393337353032'
[  +0.006365] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000786] FS-Cache: N-cookie d=00000000f430af4f{9P.session} n=00000000e26bedc2
[  +0.001005] FS-Cache: N-key=[10] '34323934393337353032'
[  +0.410280] FS-Cache: Duplicate cookie detected
[  +0.000666] FS-Cache: O-cookie c=00000008 [p=00000002 fl=222 nc=0 na=1]
[  +0.000782] FS-Cache: O-cookie d=00000000f430af4f{9P.session} n=00000000cc53a2fb
[  +0.001310] FS-Cache: O-key=[10] '34323934393337353434'
[  +0.000713] FS-Cache: N-cookie c=00000009 [p=00000002 fl=2 nc=0 na=1]
[  +0.000850] FS-Cache: N-cookie d=00000000f430af4f{9P.session} n=000000004aa2b365
[  +0.000959] FS-Cache: N-key=[10] '34323934393337353434'
[  +0.022271] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.028350] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.258719] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.011204] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000805] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000492] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000634] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.702121] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.008015] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.018569] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.048682] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.010870] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.266026] Failed to connect to bus: No such file or directory
[  +0.825362] systemd-journald[42]: File /var/log/journal/4665820c6bb04a4ca735b49af51fb034/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.179636] new mount options do not match the existing superblock, will be ignored
[  +0.011040] netlink: 'init': attribute type 4 has an invalid length.
[  +2.118154] WSL (177) ERROR: CheckConnection: getaddrinfo() failed: -5
[Apr 6 05:46] tmpfs: Unknown parameter 'noswap'


==> etcd [574cb448b39f] <==
{"level":"info","ts":"2025-04-06T05:21:20.217300Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-04-06T05:21:20.218693Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-06T05:21:20.218738Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-06T05:21:20.218881Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-06T05:21:20.219447Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-04-06T05:21:20.219566Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-04-06T05:21:20.222378Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.465079ms"}
{"level":"info","ts":"2025-04-06T05:21:20.228437Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-04-06T05:21:20.228528Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-04-06T05:21:20.228636Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-04-06T05:21:20.228672Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-04-06T05:21:20.228679Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-04-06T05:21:20.228716Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-04-06T05:21:20.232441Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-04-06T05:21:20.296813Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-04-06T05:21:20.307887Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-04-06T05:21:20.321652Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-04-06T05:21:20.321980Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-04-06T05:21:20.322186Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-06T05:21:20.333544Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-06T05:21:20.333591Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-06T05:21:20.325368Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-04-06T05:21:20.333895Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-06T05:21:20.331265Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-06T05:21:20.335465Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-06T05:21:20.335747Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-04-06T05:21:20.335768Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-04-06T05:21:20.335871Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-06T05:21:20.335883Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-06T05:21:20.829065Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-04-06T05:21:20.829111Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-04-06T05:21:20.829125Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-04-06T05:21:20.829152Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-04-06T05:21:20.829158Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-04-06T05:21:20.829166Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-04-06T05:21:20.829172Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-04-06T05:21:20.841796Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-06T05:21:20.842800Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-04-06T05:21:20.842835Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-06T05:21:20.842974Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-06T05:21:20.843111Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-04-06T05:21:20.843661Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-04-06T05:21:20.843973Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-06T05:21:20.844058Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-06T05:21:20.844701Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-06T05:21:20.845420Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-06T05:21:20.845948Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-06T05:21:20.847788Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-04-06T05:21:20.848945Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-04-06T05:31:21.320691Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":642}
{"level":"info","ts":"2025-04-06T05:31:21.326206Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":642,"took":"4.711658ms","hash":2323451603,"current-db-size-bytes":1380352,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1380352,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-04-06T05:31:21.326263Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2323451603,"revision":642,"compact-revision":-1}
{"level":"info","ts":"2025-04-06T05:33:38.535958Z","caller":"traceutil/trace.go:171","msg":"trace[1143294102] transaction","detail":"{read_only:false; response_revision:992; number_of_response:1; }","duration":"100.933152ms","start":"2025-04-06T05:33:38.434727Z","end":"2025-04-06T05:33:38.535660Z","steps":["trace[1143294102] 'process raft request'  (duration: 100.820366ms)"],"step_count":1}
{"level":"info","ts":"2025-04-06T05:36:21.310948Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":882}
{"level":"info","ts":"2025-04-06T05:36:21.314645Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":882,"took":"3.103233ms","hash":2675188786,"current-db-size-bytes":1380352,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1089536,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2025-04-06T05:36:21.314689Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2675188786,"revision":882,"compact-revision":642}
{"level":"info","ts":"2025-04-06T05:36:34.253178Z","caller":"traceutil/trace.go:171","msg":"trace[1789125358] transaction","detail":"{read_only:false; response_revision:1241; number_of_response:1; }","duration":"116.167966ms","start":"2025-04-06T05:36:34.135639Z","end":"2025-04-06T05:36:34.251807Z","steps":["trace[1789125358] 'process raft request'  (duration: 116.035005ms)"],"step_count":1}
{"level":"info","ts":"2025-04-06T05:41:21.319078Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1223}
{"level":"info","ts":"2025-04-06T05:41:21.326781Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1223,"took":"6.983688ms","hash":1199818420,"current-db-size-bytes":1380352,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1171456,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-04-06T05:41:21.326861Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1199818420,"revision":1223,"compact-revision":882}


==> etcd [72a6e86074c9] <==
{"level":"warn","ts":"2025-04-06T05:46:58.854136Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-06T05:46:58.855226Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-04-06T05:46:58.855338Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-04-06T05:46:58.855369Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-06T05:46:58.855376Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-06T05:46:58.855429Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-06T05:46:58.859837Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-04-06T05:46:58.860095Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-04-06T05:46:58.879114Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"18.80083ms"}
{"level":"info","ts":"2025-04-06T05:46:58.899117Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2025-04-06T05:46:58.908874Z","caller":"etcdserver/raft.go:540","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":1878}
{"level":"info","ts":"2025-04-06T05:46:58.909226Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-04-06T05:46:58.909297Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2025-04-06T05:46:58.909326Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 1878, applied: 0, lastindex: 1878, lastterm: 2]"}
{"level":"warn","ts":"2025-04-06T05:46:58.910928Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-04-06T05:46:58.912008Z","caller":"mvcc/kvstore.go:346","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":1223}
{"level":"info","ts":"2025-04-06T05:46:58.915045Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1593}
{"level":"info","ts":"2025-04-06T05:46:58.916846Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-04-06T05:46:58.918659Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-04-06T05:46:58.918955Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-04-06T05:46:58.919017Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-04-06T05:46:58.919143Z","caller":"etcdserver/server.go:773","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-04-06T05:46:58.919427Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-04-06T05:46:58.919535Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-06T05:46:58.919626Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-06T05:46:58.919666Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-06T05:46:58.919900Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-06T05:46:58.920189Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-06T05:46:58.920204Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-06T05:46:58.922199Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-06T05:46:58.924059Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-06T05:46:58.924263Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-06T05:46:58.924325Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-06T05:46:58.926037Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-04-06T05:46:58.926097Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-04-06T05:47:00.709737Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-04-06T05:47:00.709995Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-04-06T05:47:00.710020Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-04-06T05:47:00.710038Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-04-06T05:47:00.710045Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-04-06T05:47:00.710135Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-04-06T05:47:00.710145Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-04-06T05:47:00.712556Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-04-06T05:47:00.712583Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-06T05:47:00.712599Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-06T05:47:00.712908Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-04-06T05:47:00.712945Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-04-06T05:47:00.714389Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-06T05:47:00.714404Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-06T05:47:00.714867Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-04-06T05:47:00.715351Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-04-06T05:47:02.204297Z","caller":"traceutil/trace.go:171","msg":"trace[1107772139] transaction","detail":"{read_only:false; number_of_response:0; response_revision:1597; }","duration":"102.757506ms","start":"2025-04-06T05:47:02.101524Z","end":"2025-04-06T05:47:02.204281Z","steps":["trace[1107772139] 'process raft request'  (duration: 102.636932ms)"],"step_count":1}
{"level":"info","ts":"2025-04-06T05:47:02.204362Z","caller":"traceutil/trace.go:171","msg":"trace[2063831420] transaction","detail":"{read_only:false; number_of_response:0; response_revision:1597; }","duration":"102.716297ms","start":"2025-04-06T05:47:02.101631Z","end":"2025-04-06T05:47:02.204347Z","steps":["trace[2063831420] 'process raft request'  (duration: 102.615951ms)"],"step_count":1}
{"level":"info","ts":"2025-04-06T05:47:02.204457Z","caller":"traceutil/trace.go:171","msg":"trace[1710412248] transaction","detail":"{read_only:false; number_of_response:0; response_revision:1597; }","duration":"102.852541ms","start":"2025-04-06T05:47:02.101600Z","end":"2025-04-06T05:47:02.204452Z","steps":["trace[1710412248] 'process raft request'  (duration: 102.632057ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-06T05:47:02.204533Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.095347ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/default/default\" limit:1 ","response":"range_response_count:1 size:171"}
{"level":"info","ts":"2025-04-06T05:47:02.204575Z","caller":"traceutil/trace.go:171","msg":"trace[2049825223] range","detail":"{range_begin:/registry/serviceaccounts/default/default; range_end:; response_count:1; response_revision:1597; }","duration":"100.189247ms","start":"2025-04-06T05:47:02.104376Z","end":"2025-04-06T05:47:02.204565Z","steps":["trace[2049825223] 'agreement among raft nodes before linearized reading'  (duration: 100.024435ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-06T05:47:02.208837Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.573992ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/default/default\" limit:1 ","response":"range_response_count:1 size:171"}
{"level":"info","ts":"2025-04-06T05:47:02.208905Z","caller":"traceutil/trace.go:171","msg":"trace[2111379717] range","detail":"{range_begin:/registry/serviceaccounts/default/default; range_end:; response_count:1; response_revision:1597; }","duration":"101.668267ms","start":"2025-04-06T05:47:02.107224Z","end":"2025-04-06T05:47:02.208892Z","steps":["trace[2111379717] 'agreement among raft nodes before linearized reading'  (duration: 101.567167ms)"],"step_count":1}
{"level":"info","ts":"2025-04-06T05:47:02.412087Z","caller":"traceutil/trace.go:171","msg":"trace[569951273] transaction","detail":"{read_only:false; number_of_response:0; response_revision:1598; }","duration":"109.884437ms","start":"2025-04-06T05:47:02.302186Z","end":"2025-04-06T05:47:02.412070Z","steps":["trace[569951273] 'process raft request'  (duration: 109.821006ms)"],"step_count":1}


==> kernel <==
 05:52:24 up 6 min,  0 users,  load average: 0.40, 0.53, 0.31
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [1d57265cc2c8] <==
I0406 05:21:22.137439       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0406 05:21:22.137873       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0406 05:21:22.137912       1 aggregator.go:169] waiting for initial CRD sync...
I0406 05:21:22.137922       1 controller.go:78] Starting OpenAPI AggregationController
I0406 05:21:22.146638       1 local_available_controller.go:156] Starting LocalAvailability controller
I0406 05:21:22.146679       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0406 05:21:22.146819       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0406 05:21:22.146857       1 controller.go:142] Starting OpenAPI controller
I0406 05:21:22.146879       1 controller.go:90] Starting OpenAPI V3 controller
I0406 05:21:22.146897       1 naming_controller.go:294] Starting NamingConditionController
I0406 05:21:22.146914       1 establishing_controller.go:81] Starting EstablishingController
I0406 05:21:22.146928       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0406 05:21:22.146960       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0406 05:21:22.146972       1 crd_finalizer.go:269] Starting CRDFinalizer
I0406 05:21:22.182824       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0406 05:21:22.182851       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0406 05:21:22.182916       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0406 05:21:22.184374       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0406 05:21:22.184528       1 controller.go:119] Starting legacy_token_tracking_controller
I0406 05:21:22.184535       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0406 05:21:22.190099       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0406 05:21:22.190238       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0406 05:21:22.190329       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0406 05:21:22.190482       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0406 05:21:22.293412       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0406 05:21:22.293528       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0406 05:21:22.293843       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0406 05:21:22.293894       1 aggregator.go:171] initial CRD sync complete...
I0406 05:21:22.293908       1 autoregister_controller.go:144] Starting autoregister controller
I0406 05:21:22.293918       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0406 05:21:22.293928       1 cache.go:39] Caches are synced for autoregister controller
I0406 05:21:22.294040       1 cache.go:39] Caches are synced for LocalAvailability controller
I0406 05:21:22.296413       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0406 05:21:22.301451       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0406 05:21:22.397317       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0406 05:21:22.397441       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0406 05:21:22.398793       1 shared_informer.go:320] Caches are synced for configmaps
I0406 05:21:22.401164       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0406 05:21:22.401194       1 policy_source.go:240] refreshing policies
I0406 05:21:22.413354       1 shared_informer.go:320] Caches are synced for node_authorizer
I0406 05:21:22.517373       1 controller.go:615] quota admission added evaluator for: namespaces
E0406 05:21:22.634970       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E0406 05:21:22.636021       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0406 05:21:22.839916       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0406 05:21:23.143916       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0406 05:21:23.150828       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0406 05:21:23.150860       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0406 05:21:23.710439       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0406 05:21:23.749491       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0406 05:21:23.823300       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0406 05:21:23.830001       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0406 05:21:23.831096       1 controller.go:615] quota admission added evaluator for: endpoints
I0406 05:21:23.835573       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0406 05:21:24.321461       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0406 05:21:24.958791       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0406 05:21:24.978965       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0406 05:21:24.999507       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0406 05:21:29.723490       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0406 05:21:29.770082       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0406 05:34:25.787792       1 alloc.go:330] "allocated clusterIPs" service="default/hello-flask-change-service" clusterIPs={"IPv4":"10.97.140.158"}


==> kube-apiserver [7bdf3a07b56b] <==
W0406 05:47:01.137133       1 genericapiserver.go:767] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0406 05:47:01.528005       1 secure_serving.go:213] Serving securely on [::]:8443
I0406 05:47:01.528197       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0406 05:47:01.528257       1 controller.go:78] Starting OpenAPI AggregationController
I0406 05:47:01.528392       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0406 05:47:01.528421       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0406 05:47:01.528704       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0406 05:47:01.528758       1 controller.go:142] Starting OpenAPI controller
I0406 05:47:01.528876       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0406 05:47:01.528919       1 aggregator.go:169] waiting for initial CRD sync...
I0406 05:47:01.528931       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0406 05:47:01.529098       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0406 05:47:01.529181       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0406 05:47:01.529350       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0406 05:47:01.529376       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0406 05:47:01.529413       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0406 05:47:01.529456       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0406 05:47:01.531248       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0406 05:47:01.531362       1 controller.go:119] Starting legacy_token_tracking_controller
I0406 05:47:01.531372       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0406 05:47:01.531422       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0406 05:47:01.532236       1 establishing_controller.go:81] Starting EstablishingController
I0406 05:47:01.532306       1 controller.go:90] Starting OpenAPI V3 controller
I0406 05:47:01.532325       1 naming_controller.go:294] Starting NamingConditionController
I0406 05:47:01.560863       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0406 05:47:01.560916       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0406 05:47:01.560928       1 crd_finalizer.go:269] Starting CRDFinalizer
I0406 05:47:01.560999       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0406 05:47:01.561034       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0406 05:47:01.561057       1 local_available_controller.go:156] Starting LocalAvailability controller
I0406 05:47:01.561060       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0406 05:47:01.561825       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0406 05:47:01.561875       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0406 05:47:01.561922       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0406 05:47:01.564238       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0406 05:47:01.791075       1 cache.go:39] Caches are synced for LocalAvailability controller
I0406 05:47:01.791313       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0406 05:47:01.791744       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0406 05:47:01.791755       1 policy_source.go:240] refreshing policies
I0406 05:47:01.792386       1 shared_informer.go:320] Caches are synced for configmaps
I0406 05:47:01.792503       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0406 05:47:01.792752       1 aggregator.go:171] initial CRD sync complete...
I0406 05:47:01.792759       1 autoregister_controller.go:144] Starting autoregister controller
I0406 05:47:01.792765       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0406 05:47:01.792769       1 cache.go:39] Caches are synced for autoregister controller
I0406 05:47:01.792891       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0406 05:47:01.792897       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0406 05:47:01.793869       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0406 05:47:01.808480       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0406 05:47:01.809779       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0406 05:47:01.891620       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0406 05:47:01.891843       1 shared_informer.go:320] Caches are synced for node_authorizer
E0406 05:47:01.995996       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0406 05:47:02.091054       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0406 05:47:02.644940       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0406 05:47:03.529180       1 controller.go:615] quota admission added evaluator for: endpoints
I0406 05:47:05.316230       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0406 05:47:05.563117       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0406 05:47:05.611456       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0406 05:47:05.695689       1 controller.go:615] quota admission added evaluator for: replicasets.apps


==> kube-controller-manager [c13c74f2a921] <==
I0406 05:47:05.117414       1 shared_informer.go:320] Caches are synced for node
I0406 05:47:05.117442       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0406 05:47:05.117575       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0406 05:47:05.117604       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0406 05:47:05.117610       1 shared_informer.go:320] Caches are synced for cidrallocator
I0406 05:47:05.117829       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0406 05:47:05.136877       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0406 05:47:05.190778       1 shared_informer.go:320] Caches are synced for TTL after finished
I0406 05:47:05.191127       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0406 05:47:05.191323       1 shared_informer.go:320] Caches are synced for crt configmap
I0406 05:47:05.191847       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0406 05:47:05.191931       1 shared_informer.go:320] Caches are synced for expand
I0406 05:47:05.195126       1 shared_informer.go:320] Caches are synced for TTL
I0406 05:47:05.196871       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0406 05:47:05.197969       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0406 05:47:05.204159       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0406 05:47:05.206723       1 shared_informer.go:320] Caches are synced for service account
I0406 05:47:05.208279       1 shared_informer.go:320] Caches are synced for persistent volume
I0406 05:47:05.208477       1 shared_informer.go:320] Caches are synced for taint
I0406 05:47:05.208522       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0406 05:47:05.208574       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0406 05:47:05.209128       1 shared_informer.go:320] Caches are synced for cronjob
I0406 05:47:05.209658       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0406 05:47:05.290720       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0406 05:47:05.291464       1 shared_informer.go:320] Caches are synced for namespace
I0406 05:47:05.291506       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0406 05:47:05.291743       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0406 05:47:05.291766       1 shared_informer.go:320] Caches are synced for ReplicationController
I0406 05:47:05.291783       1 shared_informer.go:320] Caches are synced for PVC protection
I0406 05:47:05.291805       1 shared_informer.go:320] Caches are synced for daemon sets
I0406 05:47:05.292033       1 shared_informer.go:320] Caches are synced for ephemeral
I0406 05:47:05.292081       1 shared_informer.go:320] Caches are synced for stateful set
I0406 05:47:05.307782       1 shared_informer.go:320] Caches are synced for disruption
I0406 05:47:05.309797       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0406 05:47:05.310390       1 shared_informer.go:320] Caches are synced for endpoint
I0406 05:47:05.310743       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0406 05:47:05.310867       1 shared_informer.go:320] Caches are synced for resource quota
I0406 05:47:05.310877       1 shared_informer.go:320] Caches are synced for attach detach
I0406 05:47:05.311200       1 shared_informer.go:320] Caches are synced for deployment
I0406 05:47:05.313299       1 shared_informer.go:320] Caches are synced for GC
I0406 05:47:05.313371       1 shared_informer.go:320] Caches are synced for job
I0406 05:47:05.313427       1 shared_informer.go:320] Caches are synced for garbage collector
I0406 05:47:05.313434       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0406 05:47:05.313441       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0406 05:47:05.313523       1 shared_informer.go:320] Caches are synced for PV protection
I0406 05:47:05.313548       1 shared_informer.go:320] Caches are synced for HPA
I0406 05:47:05.313946       1 shared_informer.go:320] Caches are synced for garbage collector
I0406 05:47:05.313975       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0406 05:47:05.314029       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="29.997Âµs"
I0406 05:47:05.314107       1 shared_informer.go:320] Caches are synced for resource quota
I0406 05:47:05.700481       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="386.420994ms"
I0406 05:47:05.700579       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="30.883Âµs"
I0406 05:47:06.461688       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="42.835Âµs"
I0406 05:47:06.477247       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="40.562Âµs"
I0406 05:47:06.501134       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="36.404Âµs"
I0406 05:47:06.513970       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="42.267Âµs"
I0406 05:47:07.519959       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="47.735Âµs"
I0406 05:47:17.535238       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="14.072155ms"
I0406 05:47:17.535382       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="62.531Âµs"
I0406 05:51:16.789775       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [f91627437b1f] <==
I0406 05:21:28.893008       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0406 05:21:28.893060       1 shared_informer.go:320] Caches are synced for expand
I0406 05:21:28.893147       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0406 05:21:28.893182       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0406 05:21:28.893439       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0406 05:21:28.893459       1 shared_informer.go:320] Caches are synced for disruption
I0406 05:21:28.893496       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0406 05:21:28.893511       1 shared_informer.go:320] Caches are synced for namespace
I0406 05:21:28.893447       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0406 05:21:28.893580       1 shared_informer.go:320] Caches are synced for PV protection
I0406 05:21:28.893584       1 shared_informer.go:320] Caches are synced for resource quota
I0406 05:21:28.893589       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0406 05:21:28.893607       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0406 05:21:28.893636       1 shared_informer.go:320] Caches are synced for resource quota
I0406 05:21:28.893662       1 shared_informer.go:320] Caches are synced for HPA
I0406 05:21:28.893674       1 shared_informer.go:320] Caches are synced for service account
I0406 05:21:28.893786       1 shared_informer.go:320] Caches are synced for deployment
I0406 05:21:28.893928       1 shared_informer.go:320] Caches are synced for daemon sets
I0406 05:21:28.893952       1 shared_informer.go:320] Caches are synced for PVC protection
I0406 05:21:28.893959       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0406 05:21:28.901991       1 shared_informer.go:320] Caches are synced for stateful set
I0406 05:21:28.902301       1 shared_informer.go:320] Caches are synced for endpoint
I0406 05:21:28.909285       1 shared_informer.go:320] Caches are synced for node
I0406 05:21:28.909345       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0406 05:21:28.909364       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0406 05:21:28.909369       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0406 05:21:28.909374       1 shared_informer.go:320] Caches are synced for cidrallocator
I0406 05:21:28.917953       1 shared_informer.go:320] Caches are synced for garbage collector
I0406 05:21:28.917984       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0406 05:21:28.917991       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0406 05:21:28.936875       1 shared_informer.go:320] Caches are synced for ReplicationController
I0406 05:21:28.936888       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0406 05:21:28.937093       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0406 05:21:28.937112       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0406 05:21:28.937333       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0406 05:21:28.937361       1 shared_informer.go:320] Caches are synced for GC
I0406 05:21:28.937363       1 shared_informer.go:320] Caches are synced for garbage collector
I0406 05:21:28.937387       1 shared_informer.go:320] Caches are synced for ephemeral
I0406 05:21:29.482789       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0406 05:21:29.991981       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="263.46354ms"
I0406 05:21:30.004379       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="12.346847ms"
I0406 05:21:30.004465       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="42.472Âµs"
I0406 05:21:30.011597       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="58.488Âµs"
I0406 05:21:31.284906       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="131.379Âµs"
I0406 05:21:35.462291       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0406 05:21:46.138381       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="9.414744ms"
I0406 05:21:46.139957       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="56.858Âµs"
I0406 05:24:39.514322       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0406 05:29:45.674906       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0406 05:34:01.014831       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0406 05:34:25.842708       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="34.682247ms"
I0406 05:34:25.870404       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="27.618445ms"
I0406 05:34:25.870501       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="40.579Âµs"
I0406 05:34:25.874023       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="48.764Âµs"
I0406 05:34:25.886166       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="46.963Âµs"
I0406 05:34:27.136553       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="48.3Âµs"
I0406 05:34:27.148634       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="40.336Âµs"
I0406 05:34:27.159490       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-python-f4bf545" duration="40.137Âµs"
I0406 05:38:26.984951       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0406 05:43:32.655251       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [86144e9c294b] <==
I0406 05:47:05.766851       1 server_linux.go:66] "Using iptables proxy"
I0406 05:47:06.032979       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0406 05:47:06.033058       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0406 05:47:06.052164       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0406 05:47:06.052214       1 server_linux.go:170] "Using iptables Proxier"
I0406 05:47:06.054421       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0406 05:47:06.062704       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0406 05:47:06.071052       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0406 05:47:06.071566       1 server.go:497] "Version info" version="v1.32.0"
I0406 05:47:06.071609       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0406 05:47:06.081987       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0406 05:47:06.090130       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0406 05:47:06.094415       1 config.go:329] "Starting node config controller"
I0406 05:47:06.094430       1 config.go:105] "Starting endpoint slice config controller"
I0406 05:47:06.094905       1 shared_informer.go:313] Waiting for caches to sync for node config
I0406 05:47:06.094440       1 config.go:199] "Starting service config controller"
I0406 05:47:06.094971       1 shared_informer.go:313] Waiting for caches to sync for service config
I0406 05:47:06.095128       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0406 05:47:06.195786       1 shared_informer.go:320] Caches are synced for node config
I0406 05:47:06.195842       1 shared_informer.go:320] Caches are synced for service config
I0406 05:47:06.195859       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [d7befca2866b] <==
I0406 05:21:30.541614       1 server_linux.go:66] "Using iptables proxy"
I0406 05:21:30.787134       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0406 05:21:30.787207       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0406 05:21:30.805753       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0406 05:21:30.805843       1 server_linux.go:170] "Using iptables Proxier"
I0406 05:21:30.808351       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0406 05:21:30.821863       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0406 05:21:30.837867       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0406 05:21:30.838197       1 server.go:497] "Version info" version="v1.32.0"
I0406 05:21:30.838261       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0406 05:21:30.847085       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0406 05:21:30.854990       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0406 05:21:30.856072       1 config.go:105] "Starting endpoint slice config controller"
I0406 05:21:30.856114       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0406 05:21:30.856123       1 config.go:329] "Starting node config controller"
I0406 05:21:30.856137       1 shared_informer.go:313] Waiting for caches to sync for node config
I0406 05:21:30.856152       1 config.go:199] "Starting service config controller"
I0406 05:21:30.856156       1 shared_informer.go:313] Waiting for caches to sync for service config
I0406 05:21:30.956525       1 shared_informer.go:320] Caches are synced for node config
I0406 05:21:30.956569       1 shared_informer.go:320] Caches are synced for service config
I0406 05:21:30.956623       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [6b6de4e73268] <==
I0406 05:46:59.311610       1 serving.go:386] Generated self-signed cert in-memory
W0406 05:47:01.793598       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0406 05:47:01.793697       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0406 05:47:01.793710       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0406 05:47:01.793716       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0406 05:47:01.914236       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0406 05:47:01.914277       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0406 05:47:02.011389       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0406 05:47:02.011894       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0406 05:47:02.012100       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0406 05:47:02.097243       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0406 05:47:02.397313       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [e94a8930eece] <==
I0406 05:21:20.899674       1 serving.go:386] Generated self-signed cert in-memory
W0406 05:21:22.218884       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0406 05:21:22.218937       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0406 05:21:22.218949       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0406 05:21:22.218956       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0406 05:21:22.622373       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0406 05:21:22.622436       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0406 05:21:22.624838       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0406 05:21:22.624892       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0406 05:21:22.624977       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
W0406 05:21:22.627266       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0406 05:21:22.627491       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I0406 05:21:22.627638       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0406 05:21:22.629976       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0406 05:21:22.630014       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0406 05:21:22.630067       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0406 05:21:22.630077       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0406 05:21:22.630123       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0406 05:21:22.630131       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0406 05:21:22.630188       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0406 05:21:22.630195       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0406 05:21:22.630182       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0406 05:21:22.630211       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0406 05:21:22.630217       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0406 05:21:22.630220       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0406 05:21:22.630236       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0406 05:21:22.630236       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0406 05:21:22.630252       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0406 05:21:22.630261       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0406 05:21:22.630294       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0406 05:21:22.630307       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0406 05:21:22.630308       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0406 05:21:22.630314       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0406 05:21:22.630295       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
W0406 05:21:22.630324       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0406 05:21:22.630333       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0406 05:21:22.631210       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0406 05:21:22.631235       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0406 05:21:22.630326       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0406 05:21:22.631617       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0406 05:21:22.631629       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0406 05:21:22.631649       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0406 05:21:22.631649       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0406 05:21:23.490628       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0406 05:21:23.490680       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
I0406 05:21:24.025959       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Apr 06 05:50:22 minikube kubelet[1630]: E0406 05:50:22.983446    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf4tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-clqf8_default(a238f437-a799-4246-a617-868625ad1f64): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:50:22 minikube kubelet[1630]: E0406 05:50:22.984535    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-clqf8" podUID="a238f437-a799-4246-a617-868625ad1f64"
Apr 06 05:50:24 minikube kubelet[1630]: E0406 05:50:24.983272    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbkpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-pd774_default(ac46ea6b-9ee3-4aa9-a555-f8dae0147478): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:50:24 minikube kubelet[1630]: E0406 05:50:24.984364    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-pd774" podUID="ac46ea6b-9ee3-4aa9-a555-f8dae0147478"
Apr 06 05:50:27 minikube kubelet[1630]: E0406 05:50:27.983832    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfwkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-kjgvt_default(76e29e9e-08c7-4103-9650-a87957c14c02): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:50:27 minikube kubelet[1630]: E0406 05:50:27.985055    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-kjgvt" podUID="76e29e9e-08c7-4103-9650-a87957c14c02"
Apr 06 05:50:36 minikube kubelet[1630]: E0406 05:50:36.992465    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf4tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-clqf8_default(a238f437-a799-4246-a617-868625ad1f64): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:50:36 minikube kubelet[1630]: E0406 05:50:36.993611    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-clqf8" podUID="a238f437-a799-4246-a617-868625ad1f64"
Apr 06 05:50:36 minikube kubelet[1630]: E0406 05:50:36.992187    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbkpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-pd774_default(ac46ea6b-9ee3-4aa9-a555-f8dae0147478): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:50:36 minikube kubelet[1630]: E0406 05:50:36.994937    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-pd774" podUID="ac46ea6b-9ee3-4aa9-a555-f8dae0147478"
Apr 06 05:50:38 minikube kubelet[1630]: E0406 05:50:38.984642    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfwkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-kjgvt_default(76e29e9e-08c7-4103-9650-a87957c14c02): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:50:38 minikube kubelet[1630]: E0406 05:50:38.985841    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-kjgvt" podUID="76e29e9e-08c7-4103-9650-a87957c14c02"
Apr 06 05:50:48 minikube kubelet[1630]: E0406 05:50:48.984898    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf4tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-clqf8_default(a238f437-a799-4246-a617-868625ad1f64): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:50:48 minikube kubelet[1630]: E0406 05:50:48.986503    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-clqf8" podUID="a238f437-a799-4246-a617-868625ad1f64"
Apr 06 05:50:50 minikube kubelet[1630]: E0406 05:50:50.982487    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfwkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-kjgvt_default(76e29e9e-08c7-4103-9650-a87957c14c02): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:50:50 minikube kubelet[1630]: E0406 05:50:50.983610    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-kjgvt" podUID="76e29e9e-08c7-4103-9650-a87957c14c02"
Apr 06 05:50:51 minikube kubelet[1630]: E0406 05:50:51.982269    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbkpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-pd774_default(ac46ea6b-9ee3-4aa9-a555-f8dae0147478): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:50:51 minikube kubelet[1630]: E0406 05:50:51.983480    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-pd774" podUID="ac46ea6b-9ee3-4aa9-a555-f8dae0147478"
Apr 06 05:51:01 minikube kubelet[1630]: E0406 05:51:01.982651    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf4tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-clqf8_default(a238f437-a799-4246-a617-868625ad1f64): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:01 minikube kubelet[1630]: E0406 05:51:01.983889    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-clqf8" podUID="a238f437-a799-4246-a617-868625ad1f64"
Apr 06 05:51:03 minikube kubelet[1630]: E0406 05:51:03.982571    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbkpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-pd774_default(ac46ea6b-9ee3-4aa9-a555-f8dae0147478): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:03 minikube kubelet[1630]: E0406 05:51:03.983778    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-pd774" podUID="ac46ea6b-9ee3-4aa9-a555-f8dae0147478"
Apr 06 05:51:04 minikube kubelet[1630]: E0406 05:51:04.982183    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfwkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-kjgvt_default(76e29e9e-08c7-4103-9650-a87957c14c02): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:04 minikube kubelet[1630]: E0406 05:51:04.984028    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-kjgvt" podUID="76e29e9e-08c7-4103-9650-a87957c14c02"
Apr 06 05:51:12 minikube kubelet[1630]: E0406 05:51:12.982266    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf4tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-clqf8_default(a238f437-a799-4246-a617-868625ad1f64): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:12 minikube kubelet[1630]: E0406 05:51:12.983449    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-clqf8" podUID="a238f437-a799-4246-a617-868625ad1f64"
Apr 06 05:51:17 minikube kubelet[1630]: E0406 05:51:17.980949    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbkpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-pd774_default(ac46ea6b-9ee3-4aa9-a555-f8dae0147478): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:17 minikube kubelet[1630]: E0406 05:51:17.982180    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-pd774" podUID="ac46ea6b-9ee3-4aa9-a555-f8dae0147478"
Apr 06 05:51:18 minikube kubelet[1630]: E0406 05:51:18.980564    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfwkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-kjgvt_default(76e29e9e-08c7-4103-9650-a87957c14c02): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:18 minikube kubelet[1630]: E0406 05:51:18.981764    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-kjgvt" podUID="76e29e9e-08c7-4103-9650-a87957c14c02"
Apr 06 05:51:24 minikube kubelet[1630]: E0406 05:51:24.980817    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf4tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-clqf8_default(a238f437-a799-4246-a617-868625ad1f64): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:24 minikube kubelet[1630]: E0406 05:51:24.981975    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-clqf8" podUID="a238f437-a799-4246-a617-868625ad1f64"
Apr 06 05:51:31 minikube kubelet[1630]: E0406 05:51:31.980221    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbkpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-pd774_default(ac46ea6b-9ee3-4aa9-a555-f8dae0147478): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:31 minikube kubelet[1630]: E0406 05:51:31.980230    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfwkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-kjgvt_default(76e29e9e-08c7-4103-9650-a87957c14c02): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:31 minikube kubelet[1630]: E0406 05:51:31.981731    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-kjgvt" podUID="76e29e9e-08c7-4103-9650-a87957c14c02"
Apr 06 05:51:31 minikube kubelet[1630]: E0406 05:51:31.981760    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-pd774" podUID="ac46ea6b-9ee3-4aa9-a555-f8dae0147478"
Apr 06 05:51:36 minikube kubelet[1630]: E0406 05:51:36.981176    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf4tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-clqf8_default(a238f437-a799-4246-a617-868625ad1f64): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:36 minikube kubelet[1630]: E0406 05:51:36.983259    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-clqf8" podUID="a238f437-a799-4246-a617-868625ad1f64"
Apr 06 05:51:43 minikube kubelet[1630]: E0406 05:51:43.980382    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbkpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-pd774_default(ac46ea6b-9ee3-4aa9-a555-f8dae0147478): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:43 minikube kubelet[1630]: E0406 05:51:43.981596    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-pd774" podUID="ac46ea6b-9ee3-4aa9-a555-f8dae0147478"
Apr 06 05:51:46 minikube kubelet[1630]: E0406 05:51:46.982491    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfwkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-kjgvt_default(76e29e9e-08c7-4103-9650-a87957c14c02): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:46 minikube kubelet[1630]: E0406 05:51:46.983715    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-kjgvt" podUID="76e29e9e-08c7-4103-9650-a87957c14c02"
Apr 06 05:51:51 minikube kubelet[1630]: E0406 05:51:51.979067    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf4tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-clqf8_default(a238f437-a799-4246-a617-868625ad1f64): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:51 minikube kubelet[1630]: E0406 05:51:51.980350    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-clqf8" podUID="a238f437-a799-4246-a617-868625ad1f64"
Apr 06 05:51:57 minikube kubelet[1630]: E0406 05:51:57.978724    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfwkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-kjgvt_default(76e29e9e-08c7-4103-9650-a87957c14c02): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:57 minikube kubelet[1630]: E0406 05:51:57.978746    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbkpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-pd774_default(ac46ea6b-9ee3-4aa9-a555-f8dae0147478): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:51:57 minikube kubelet[1630]: E0406 05:51:57.980329    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-pd774" podUID="ac46ea6b-9ee3-4aa9-a555-f8dae0147478"
Apr 06 05:51:57 minikube kubelet[1630]: E0406 05:51:57.980367    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-kjgvt" podUID="76e29e9e-08c7-4103-9650-a87957c14c02"
Apr 06 05:52:02 minikube kubelet[1630]: E0406 05:52:02.979000    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf4tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-clqf8_default(a238f437-a799-4246-a617-868625ad1f64): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:52:02 minikube kubelet[1630]: E0406 05:52:02.980086    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-clqf8" podUID="a238f437-a799-4246-a617-868625ad1f64"
Apr 06 05:52:08 minikube kubelet[1630]: E0406 05:52:08.979359    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfwkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-kjgvt_default(76e29e9e-08c7-4103-9650-a87957c14c02): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:52:08 minikube kubelet[1630]: E0406 05:52:08.980555    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-kjgvt" podUID="76e29e9e-08c7-4103-9650-a87957c14c02"
Apr 06 05:52:11 minikube kubelet[1630]: E0406 05:52:11.978676    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbkpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-pd774_default(ac46ea6b-9ee3-4aa9-a555-f8dae0147478): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:52:11 minikube kubelet[1630]: E0406 05:52:11.979895    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-pd774" podUID="ac46ea6b-9ee3-4aa9-a555-f8dae0147478"
Apr 06 05:52:13 minikube kubelet[1630]: E0406 05:52:13.979368    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf4tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-clqf8_default(a238f437-a799-4246-a617-868625ad1f64): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:52:13 minikube kubelet[1630]: E0406 05:52:13.980604    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-clqf8" podUID="a238f437-a799-4246-a617-868625ad1f64"
Apr 06 05:52:19 minikube kubelet[1630]: E0406 05:52:19.976945    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfwkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-kjgvt_default(76e29e9e-08c7-4103-9650-a87957c14c02): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:52:19 minikube kubelet[1630]: E0406 05:52:19.978219    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-kjgvt" podUID="76e29e9e-08c7-4103-9650-a87957c14c02"
Apr 06 05:52:22 minikube kubelet[1630]: E0406 05:52:22.976874    1630 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:flask-change,Image:flask-change:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbkpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-python-f4bf545-pd774_default(ac46ea6b-9ee3-4aa9-a555-f8dae0147478): ErrImageNeverPull: Container image \"flask-change:latest\" is not present with pull policy of Never" logger="UnhandledError"
Apr 06 05:52:22 minikube kubelet[1630]: E0406 05:52:22.978910    1630 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-change\" with ErrImageNeverPull: \"Container image \\\"flask-change:latest\\\" is not present with pull policy of Never\"" pod="default/hello-python-f4bf545-pd774" podUID="ac46ea6b-9ee3-4aa9-a555-f8dae0147478"


==> storage-provisioner [474c436adfa0] <==
I0406 05:47:05.556808       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0406 05:47:15.608462       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout


==> storage-provisioner [e49dc451440d] <==
I0406 05:47:28.157642       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0406 05:47:28.169663       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0406 05:47:28.170250       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0406 05:47:45.579305       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0406 05:47:45.579786       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"a4c732b3-c4e5-4427-b7c7-e13afa0a94bd", APIVersion:"v1", ResourceVersion:"1766", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_5f61485f-c311-4259-8010-d5c6b4965a08 became leader
I0406 05:47:45.579946       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_5f61485f-c311-4259-8010-d5c6b4965a08!
I0406 05:47:45.685293       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_5f61485f-c311-4259-8010-d5c6b4965a08!

